{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHEME for NAACL2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-rumours  rumours\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/pheme/all-rnr-annotated-threads/charliehebdo-all-rnr-threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = [\n",
    "#             'charliehebdo',\n",
    "#             'ferguson',\n",
    "#             'ottawashooting',\n",
    "#             'putinmissing',\n",
    "#             'ebola-essien',\n",
    "#             'germanwings-crash',\n",
    "#             'prince-toronto',\n",
    "#             'sydneysiege'\n",
    "#          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''convertion code from PHEME creators. Label: True = rumour is true. False = rumour is false'''\n",
    "def convert_annotations(annotation, string = True):\n",
    "    if 'misinformation' in annotation.keys() and 'true'in annotation.keys():\n",
    "        if int(annotation['misinformation'])==0 and int(annotation['true'])==0:\n",
    "            if string:\n",
    "                label = \"unverified\"\n",
    "            else:\n",
    "                label = 2\n",
    "        elif int(annotation['misinformation'])==0 and int(annotation['true'])==1 :\n",
    "            if string:\n",
    "                label = \"true\"\n",
    "            else:\n",
    "                label = 1\n",
    "        elif int(annotation['misinformation'])==1 and int(annotation['true'])==0 :\n",
    "            if string:\n",
    "                label = \"false\"\n",
    "            else:\n",
    "                label = 0\n",
    "        elif int(annotation['misinformation'])==1 and int(annotation['true'])==1:\n",
    "            print (\"OMG! They both are 1!\")\n",
    "            print(annotation['misinformation'])\n",
    "            print(annotation['true'])\n",
    "            label = None\n",
    "            \n",
    "    elif 'misinformation' in annotation.keys() and 'true' not in annotation.keys():\n",
    "        # all instances have misinfo label but don't have true label\n",
    "        if int(annotation['misinformation'])==0:\n",
    "            if string:\n",
    "                label = \"unverified\"\n",
    "            else:\n",
    "                label = 2\n",
    "        elif int(annotation['misinformation'])==1:\n",
    "            if string:\n",
    "                label = \"false\"\n",
    "            else:\n",
    "                label = 0\n",
    "                \n",
    "    elif 'true' in annotation.keys() and 'misinformation' not in annotation.keys():\n",
    "        print ('Has true not misinformation')\n",
    "        label = None\n",
    "    else:\n",
    "        print('No annotations')\n",
    "        label = None\n",
    "           \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    thread_anno_by_events - for NAACL2021\n",
    "'''\n",
    "\n",
    "base_path = \"../data/pheme/all-rnr-annotated-threads\"\n",
    "all_event_thread_info = {}\n",
    "\n",
    "events = [d for d in os.listdir(base_path) if d!='.DS_Store']\n",
    "for event in tqdm(events):\n",
    "    threads = [d for d in os.listdir('{}/{}'.format(base_path, event))]\n",
    "    print(event, len(threads))\n",
    "\n",
    "    thread_info = {}\n",
    "    rumour_threads = [d for d in os.listdir('{}/{}/rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "\n",
    "    for r_id in rumour_threads:\n",
    "        # process lvl2 rumour veracity annotation (true, false, unverified rumour)\n",
    "        with open('{}/{}/rumours/{}/annotation.json'.format(base_path, event, r_id)) as json_file:\n",
    "            # anno fields: links - evidences/information about the rumour. category - rumour type. \n",
    "            # misinfo & true lables are used to determine labels for each thread\n",
    "            anno = json.load(json_file)\n",
    "            veracity_label = convert_annotations(anno)\n",
    "            evidence_arr = anno['links']\n",
    "            rumour_category = anno['category'] # TODO check what this is later \n",
    "\n",
    "        # combine lvl1 rumour detection labels with lvl2 labels\n",
    "        if veracity_label != 'unverified':\n",
    "            thread_info[r_id] = {\n",
    "                'thread_id': r_id,\n",
    "                'rumour_label': 'rumour',\n",
    "                'veracity_label': veracity_label,\n",
    "                'rumour_category': rumour_category\n",
    "            }\n",
    "\n",
    "    \n",
    "    event_name = event.split('-')[0]\n",
    "    all_event_thread_info[event_name]=thread_info\n",
    "\n",
    "with open('./thread_annotations_by_event.json', 'w') as outfile:\n",
    "    json.dump(all_event_thread_info, outfile)\n",
    "\n",
    "# checking for the count\n",
    "for k in all_event_thread_info:\n",
    "    print(k, len(all_event_thread_info[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sydneysiege-all-rnr-threads 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 1/9 [00:00<00:06,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ottawashooting-all-rnr-threads 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 2/9 [00:01<00:05,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebola-essien-all-rnr-threads 3\n",
      "prince-toronto-all-rnr-threads 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 4/9 [00:01<00:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ferguson-all-rnr-threads 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:02<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gurlitt-all-rnr-threads 3\n",
      "putinmissing-all-rnr-threads 3\n",
      "charliehebdo-all-rnr-threads 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 8/9 [00:02<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "germanwings-crash-all-rnr-threads 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:03<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sydneysiege 522\n",
      "ottawashooting 470\n",
      "ebola 14\n",
      "prince 229\n",
      "ferguson 284\n",
      "gurlitt 61\n",
      "putinmissing 126\n",
      "charliehebdo 458\n",
      "germanwings 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    thread_anno_by_events_three_class - for EMNLP2021\n",
    "'''\n",
    "\n",
    "base_path = \"../data/pheme/all-rnr-annotated-threads\"\n",
    "all_event_thread_info = {}\n",
    "\n",
    "events = [d for d in os.listdir(base_path) if d!='.DS_Store']\n",
    "for event in tqdm(events):\n",
    "    threads = [d for d in os.listdir('{}/{}'.format(base_path, event))]\n",
    "    print(event, len(threads))\n",
    "\n",
    "    thread_info = {}\n",
    "    rumour_threads = [d for d in os.listdir('{}/{}/rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "\n",
    "    for r_id in rumour_threads:\n",
    "        # process lvl2 rumour veracity annotation (true, false, unverified rumour)\n",
    "        with open('{}/{}/rumours/{}/annotation.json'.format(base_path, event, r_id)) as json_file:\n",
    "            # anno fields: links - evidences/information about the rumour. category - rumour type. \n",
    "            # misinfo & true lables are used to determine labels for each thread\n",
    "            anno = json.load(json_file)\n",
    "            veracity_label = convert_annotations(anno)\n",
    "            evidence_arr = anno['links']\n",
    "            rumour_category = anno['category'] # TODO check what this is later \n",
    "\n",
    "        # combine lvl1 rumour detection labels with lvl2 labels\n",
    "#         if veracity_label != 'unverified':\n",
    "        thread_info[r_id] = {\n",
    "            'thread_id': r_id,\n",
    "            'rumour_label': 'rumour',\n",
    "            'veracity_label': veracity_label,\n",
    "            'rumour_category': rumour_category\n",
    "        }\n",
    "\n",
    "    \n",
    "    event_name = event.split('-')[0]\n",
    "    all_event_thread_info[event_name]=thread_info\n",
    "\n",
    "with open('./thread_annotations_by_event_3class.json', 'w') as outfile:\n",
    "    json.dump(all_event_thread_info, outfile)\n",
    "\n",
    "# checking for the count\n",
    "for k in all_event_thread_info:\n",
    "    print(k, len(all_event_thread_info[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gurlitt',\n",
       " 'germanwings',\n",
       " 'ebola',\n",
       " 'prince',\n",
       " 'ferguson',\n",
       " 'ottawashooting',\n",
       " 'sydneysiege',\n",
       " 'charliehebdo',\n",
       " 'putinmissing']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    \"gurlitt\",\n",
    "    \"germanwings\",\n",
    "    \"ebola\",\n",
    "    \"prince\",\n",
    "    \"ferguson\",\n",
    "    \"ottawashooting\",\n",
    "    \"sydneysiege\",\n",
    "    \"charliehebdo\",\n",
    "    \"putinmissing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/preprocessed/id2text.pickle', 'rb') as handler:\n",
    "    id2data = pickle.load(handler)\n",
    "\n",
    "veracity_path = '../data/preprocessed/thread_annotations_by_event.json'\n",
    "with open(veracity_path) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# split the train/dev/test based on the event\n",
    "\n",
    "# prepare test idx\n",
    "test_event_name = 'gurlitt'\n",
    "test = list(data[test_event_name].values())\n",
    "\n",
    "# prepare train/dev idx\n",
    "train_dev_list = []\n",
    "for event_name in self.events:\n",
    "    if event_name != self.test_event_name:\n",
    "        train_dev_list.extend(list(data[event_name].values()))\n",
    "\n",
    "self.train, self.dev = train_test_split(train_dev_list, test_size=0.15, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lvl 1 rumour verification anno ( rumour vs non-rumour) AND lvl2 rumour veracity anno (true, false, unverified) from PHEME data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''convertion code from PHEME creators. Label: True = rumour is true. False = rumour is false'''\n",
    "def convert_annotations(annotation, string = True):\n",
    "    if 'misinformation' in annotation.keys() and 'true'in annotation.keys():\n",
    "        if int(annotation['misinformation'])==0 and int(annotation['true'])==0:\n",
    "            if string:\n",
    "                label = \"unverified\"\n",
    "            else:\n",
    "                label = 2\n",
    "        elif int(annotation['misinformation'])==0 and int(annotation['true'])==1 :\n",
    "            if string:\n",
    "                label = \"true\"\n",
    "            else:\n",
    "                label = 1\n",
    "        elif int(annotation['misinformation'])==1 and int(annotation['true'])==0 :\n",
    "            if string:\n",
    "                label = \"false\"\n",
    "            else:\n",
    "                label = 0\n",
    "        elif int(annotation['misinformation'])==1 and int(annotation['true'])==1:\n",
    "            print (\"OMG! They both are 1!\")\n",
    "            print(annotation['misinformation'])\n",
    "            print(annotation['true'])\n",
    "            label = None\n",
    "            \n",
    "    elif 'misinformation' in annotation.keys() and 'true' not in annotation.keys():\n",
    "        # all instances have misinfo label but don't have true label\n",
    "        if int(annotation['misinformation'])==0:\n",
    "            if string:\n",
    "                label = \"unverified\"\n",
    "            else:\n",
    "                label = 2\n",
    "        elif int(annotation['misinformation'])==1:\n",
    "            if string:\n",
    "                label = \"false\"\n",
    "            else:\n",
    "                label = 0\n",
    "                \n",
    "    elif 'true' in annotation.keys() and 'misinformation' not in annotation.keys():\n",
    "        print ('Has true not misinformation')\n",
    "        label = None\n",
    "    else:\n",
    "        print('No annotations')\n",
    "        label = None\n",
    "           \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'thread anno w/o event'\n",
    "thread_info = {}\n",
    "events = [d for d in os.listdir(base_path) if d!='.DS_Store']\n",
    "for event in tqdm(events):\n",
    "    threads = [d for d in os.listdir('{}/{}'.format(base_path, event))]\n",
    "    print(event, len(threads))\n",
    "\n",
    "    rumour_threads = [d for d in os.listdir('{}/{}/rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "    non_rumour_threads = [d for d in os.listdir('{}/{}/non-rumours'.format(base_path, event))]\n",
    "\n",
    "    for r_id in rumour_threads:\n",
    "        # process lvl2 rumour veracity annotation (true, false, unverified rumour)\n",
    "        with open('{}/{}/rumours/{}/annotation.json'.format(base_path, event, r_id)) as json_file:\n",
    "            # anno fields: links - evidences/information about the rumour. category - rumour type. \n",
    "            # misinfo & true lables are used to determine labels for each thread\n",
    "            anno = json.load(json_file)\n",
    "            veracity_label = convert_annotations(anno)\n",
    "            evidence_arr = anno['links']\n",
    "            rumour_category = anno['category'] # TODO check what this is later \n",
    "\n",
    "        # combine lvl1 rumour detection labels with lvl2 labels\n",
    "        thread_info[r_id] = {\n",
    "            'thread_id': r_id,\n",
    "            'rumour_label': 'rumour',\n",
    "            'veracity_label': veracity_label,\n",
    "            'evidence': evidence_arr,\n",
    "            'rumour_category': rumour_category\n",
    "        }\n",
    "\n",
    "    for r_id in non_rumour_threads:\n",
    "        # non-rumour doesn't have lvl2 annotation.\n",
    "        thread_info[r_id] = {'thread_id': r_id, 'rumour_label':'non-rumour'}\n",
    "    \n",
    "    event_name = event.split('-')[0]\n",
    "    \n",
    "# with open('./thread_annotations.json', 'w') as outfile:\n",
    "#     json.dump(thread_info, outfile)\n",
    "\n",
    "# # checking for the count\n",
    "# for k in thread_info:\n",
    "#     print(k, len(thread_info[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'thread_anno_by_events'\n",
    "all_event_thread_info = {}\n",
    "\n",
    "events = [d for d in os.listdir(base_path) if d!='.DS_Store']\n",
    "for event in tqdm(events):\n",
    "    threads = [d for d in os.listdir('{}/{}'.format(base_path, event))]\n",
    "    print(event, len(threads))\n",
    "\n",
    "    thread_info = {}\n",
    "    rumour_threads = [d for d in os.listdir('{}/{}/rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "    non_rumour_threads = [d for d in os.listdir('{}/{}/non-rumours'.format(base_path, event))]\n",
    "\n",
    "    for r_id in rumour_threads:\n",
    "        # process lvl2 rumour veracity annotation (true, false, unverified rumour)\n",
    "        with open('{}/{}/rumours/{}/annotation.json'.format(base_path, event, r_id)) as json_file:\n",
    "            # anno fields: links - evidences/information about the rumour. category - rumour type. \n",
    "            # misinfo & true lables are used to determine labels for each thread\n",
    "            anno = json.load(json_file)\n",
    "            veracity_label = convert_annotations(anno)\n",
    "            evidence_arr = anno['links']\n",
    "            rumour_category = anno['category'] # TODO check what this is later \n",
    "\n",
    "        # combine lvl1 rumour detection labels with lvl2 labels\n",
    "        thread_info[r_id] = {\n",
    "            'thread_id': r_id,\n",
    "            'rumour_label': 'rumour',\n",
    "            'veracity_label': veracity_label,\n",
    "            'evidence': evidence_arr,\n",
    "            'rumour_category': rumour_category\n",
    "        }\n",
    "\n",
    "    for r_id in non_rumour_threads:\n",
    "        # non-rumour doesn't have lvl2 annotation.\n",
    "        thread_info[r_id] = {'thread_id': r_id, 'rumour_label':'non-rumour'}\n",
    "    \n",
    "    event_name = event.split('-')[0]\n",
    "    all_event_thread_info[event_name]=thread_info\n",
    "    \n",
    "# with open('./thread_annotations_by_event.json', 'w') as outfile:\n",
    "#     json.dump(all_event_thread_info, outfile)\n",
    "\n",
    "# # checking for the count\n",
    "# for k in all_event_thread_info:\n",
    "#     print(k, len(all_event_thread_info[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lvl 3 tweet level annotation for Stance detection (from PHEME-journalism data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-298fd5e3cca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweet_level_annotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/pheme/annotations/en-scheme-annotations.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtweet2stance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_level_annotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root_path' is not defined"
     ]
    }
   ],
   "source": [
    "tweet_level_annotation = root_path + '/pheme/annotations/en-scheme-annotations.json'\n",
    "\n",
    "tweet2stance = defaultdict(dict)\n",
    "for _, line in enumerate(open(tweet_level_annotation, 'r')):\n",
    "    line = json.loads(line)\n",
    "    \n",
    "    threadid = line['threadid']\n",
    "    tweetid = line['tweetid']\n",
    "    \n",
    "    if threadid == tweetid: # source tweet\n",
    "        tweet2stance[threadid][tweetid] = {'thread_id': threadid, 'tweet_id': tweetid, 'event': line['event'], \n",
    "                                           'support': line['support'], 'evidence_form': line['evidentiality'], \n",
    "                                           'certainty': line['certainty']}\n",
    "    else: # replying tweet\n",
    "        tweet2stance[threadid][tweetid] = {'thread_id': threadid, 'tweet_id': tweetid, 'event': line['event'], \n",
    "                                           'stance_to_source': line['responsetype-vs-source']}\n",
    "        if 'responsetype-vs-previous' in line.keys():\n",
    "            tweet2stance[threadid][tweetid]['stance_to_prev'] = line['responsetype-vs-previous']\n",
    "        if 'evidentiality' in line.keys(): \n",
    "            tweet2stance[threadid][tweetid]['evidence_form'] = line['evidentiality']\n",
    "        if 'certainty' in line.keys():\n",
    "            tweet2stance[threadid][tweetid]['certainty'] = line['certainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tweet_annotations.json', 'wb') as outfile:\n",
    "    json.dump(tweet2stance, outfile)\n",
    "#     pickle.dump(tweet2stance, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create thread_by_event - branches from threads-structure and tweet text, grouped by events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "# taken from BranchLSTM  https://github.com/kochkinaelena/branchLSTM\n",
    "def tree2branches(root):\n",
    "    node = root\n",
    "    if len(list(node.values())[0])==0:\n",
    "        return []\n",
    "    parent_tracker = []\n",
    "    parent_tracker.append(root)\n",
    "    branch = []\n",
    "    branches = []\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        node_name = list(node.keys())[i]\n",
    "        #print node_name\n",
    "        branch.append(node_name)\n",
    "        # get children of the node\n",
    "        first_child = list(node.values())[i]\n",
    "        # actually all chldren, all tree left under this node\n",
    "        if first_child != []:  # if node has children\n",
    "            node = first_child      # walk down\n",
    "            parent_tracker.append(node)\n",
    "            siblings = list(first_child.keys())\n",
    "            i = 0  # index of a current node\n",
    "        else:\n",
    "            branches.append(deepcopy(branch))\n",
    "            i = siblings.index(node_name)  # index of a current node\n",
    "            # if the node doesn't have next siblings\n",
    "            while i+1 >= len(siblings):\n",
    "                if node is parent_tracker[0]:  # if it is a root node\n",
    "                    return branches\n",
    "                del parent_tracker[-1]\n",
    "                del branch[-1]\n",
    "                node = parent_tracker[-1]      # walk up ... one step\n",
    "                node_name = branch[-1]\n",
    "                siblings = list(node.keys())\n",
    "                i = siblings.index(node_name)\n",
    "            i = i+1    # ... walk right\n",
    "            del branch[-1]\n",
    "# branches = tree2branches(tree)\n",
    "# for b in branches:\n",
    "#     print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get general thread first from PHEME\n",
    "'Note: thread2branches_by_event - having all branches as one instance'\n",
    "# threads_by_event = {'event_name': [{'thread_id': threadid, 'branches': branch_arr}]} How it should look like\n",
    "all_event_threads = {}\n",
    "\n",
    "no_branch_cnt = 0\n",
    "yes_branch_cnt = 0\n",
    "events = [d for d in os.listdir(base_path) if d!='.DS_Store']\n",
    "for event in tqdm(events):\n",
    "    threads = []\n",
    "    rumour_threads = [d for d in os.listdir('{}/{}/rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "    non_rumour_threads = [d for d in os.listdir('{}/{}/non-rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "\n",
    "    for r_id in rumour_threads:\n",
    "        with open('{}/{}/rumours/{}/structure.json'.format(base_path, event, r_id)) as json_file:\n",
    "            thread_tree = json.load(json_file)\n",
    "            branches = tree2branches(thread_tree)\n",
    "            if len(branches)==0:\n",
    "                branches = [[r_id]]\n",
    "            threads += {'thread_id': r_id, 'branches': branches},\n",
    "\n",
    "    for r_id in non_rumour_threads:\n",
    "        with open('{}/{}/non-rumours/{}/structure.json'.format(base_path, event, r_id)) as json_file:\n",
    "            thread_tree = json.load(json_file)\n",
    "            branches = tree2branches(thread_tree)\n",
    "            if len(branches)==0:\n",
    "                branches = [[r_id]]\n",
    "            threads += {'thread_id': r_id, 'branches': branches},\n",
    "    \n",
    "    event_name = event.split('-')[0]\n",
    "    all_event_threads[event_name] = threads\n",
    "    print(event_name, len(threads))\n",
    "    \n",
    "# print(\"yes, no\", yes_branch_cnt, no_branch_cnt)\n",
    "# with open('./thread2branches_by_event.pickle', 'wb') as handle:\n",
    "#     pickle.dump(all_event_threads, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# checking for the count\n",
    "all_cnt = 0\n",
    "for k in all_event_threads:\n",
    "    all_cnt += len(all_event_threads[k])\n",
    "    print(k, len(all_event_threads[k]))\n",
    "print(all_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get general thread first from PHEME\n",
    "'Note: thread2branches_by_event_split_branch - split the branches into multiple instance'\n",
    "'Will end up with instance with same threadid but different branch'\n",
    "# threads_by_event = {'event_name': [{'thread_id': threadid, 'branches': branch_arr}]} How it should look like\n",
    "all_event_threads = {}\n",
    "\n",
    "no_branch_cnt = 0\n",
    "yes_branch_cnt = 0\n",
    "events = [d for d in os.listdir(base_path) if d!='.DS_Store']\n",
    "for event in tqdm(events):\n",
    "    threads = []\n",
    "    rumour_threads = [d for d in os.listdir('{}/{}/rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "    non_rumour_threads = [d for d in os.listdir('{}/{}/non-rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "\n",
    "    for r_id in rumour_threads:\n",
    "        with open('{}/{}/rumours/{}/structure.json'.format(base_path, event, r_id)) as json_file:\n",
    "            thread_tree = json.load(json_file)\n",
    "            branches = tree2branches(thread_tree)\n",
    "            \n",
    "            if len(branches)==0:\n",
    "                branches = [[r_id]]\n",
    "                \n",
    "            for branch in branches:\n",
    "                threads += {'thread_id': r_id, 'branch': branch},\n",
    "\n",
    "    for r_id in non_rumour_threads:\n",
    "        with open('{}/{}/non-rumours/{}/structure.json'.format(base_path, event, r_id)) as json_file:\n",
    "            thread_tree = json.load(json_file)\n",
    "            branches = tree2branches(thread_tree)\n",
    "            \n",
    "            if len(branches)==0:\n",
    "                branches = [[r_id]]\n",
    "\n",
    "            for branch in branches:\n",
    "                threads += {'thread_id': r_id, 'branch': branch},\n",
    "\n",
    "    event_name = event.split('-')[0]\n",
    "    all_event_threads[event_name] = threads\n",
    "    print(event_name, len(threads))\n",
    "    \n",
    "with open('./thread2branches_by_event_split_branch.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_event_threads, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# checking for the count\n",
    "all_cnt = 0\n",
    "for k in all_event_threads:\n",
    "    all_cnt += len(all_event_threads[k])\n",
    "    print(k, len(all_event_threads[k]))\n",
    "print(all_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID to tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_id2text(threads, lvl_1_path, id2text):\n",
    "    for r_id in threads:\n",
    "        # for each rumour_thead, there is ONE source-tweet, and MULTIPLE reations\n",
    "\n",
    "        # {base}/{event}/{rumours or non-rumour)}/{thread-id}/source-tweets/{rumour-id}.json\n",
    "        with open('{}/{}/{}/{}/source-tweets/{}.json'.format(base_path, event, lvl_1_path, r_id, r_id)) as json_file:\n",
    "            tweet_json = json.load(json_file)\n",
    "            id2text[tweet_json['id_str'].decode(\"utf-8\")] = tweet_json['text']\n",
    "            \n",
    "        reaction_tweets = [d for d in os.listdir('{}/{}/{}/{}/reactions'.format(base_path, event, lvl_1_path, r_id))\n",
    "                          if 'json' in d]\n",
    "        for r_tweet_id in reaction_tweets:\n",
    "            with open('{}/{}/{}/{}/reactions/{}'.format(base_path, event, lvl_1_path, r_id, r_tweet_id)) as json_file:\n",
    "                tweet_json = json.load(json_file)\n",
    "                id2text[tweet_json['id_str'].decode(\"utf-8\")] = tweet_json['text']\n",
    "    return id2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [d for d in os.listdir(base_path) if d!='.DS_Store']\n",
    "id2text = {}\n",
    "\n",
    "for event in tqdm(events):\n",
    "    threads = [d for d in os.listdir('{}/{}'.format(base_path, event))]\n",
    "\n",
    "    rumour_threads = [d for d in os.listdir('{}/{}/rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "    non_rumour_threads = [d for d in os.listdir('{}/{}/non-rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "\n",
    "    id2text = process_id2text(rumour_threads, 'rumours', id2text)\n",
    "    id2text = process_id2text(non_rumour_threads, 'non-rumours', id2text)\n",
    "\n",
    "# with open('./id2text.pickle', 'wb') as handle:\n",
    "#     pickle.dump(id2text, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID to tweet info mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_tweet_fields = ['text','id_str','favorite_count','retweeted','entities','retweet_count','favorited',\n",
    "                 'coordinates','created_at','place']\n",
    "# entities = entities automatically detected and extracted by twitter api - provides span and text if any detected.\n",
    "\n",
    "useful_user_fields = ['id_str', 'verified','followers_count', 'listed_count', 'statuses_count', 'description', 'friends_count', 'location',\n",
    "'name', 'lang', 'favourites_count', 'screen_name', 'url', 'created_at','time_zone']\n",
    "# listed_count = number of public lists that this user is a member of\n",
    "# statuses_count = number of tweets/retweets\n",
    "# description = self description\n",
    "# favourites_count = number of Tweets this user has liked in the account’s lifetime\n",
    "# url = user provided url associated with their profile\n",
    "\n",
    "def format_tweet(tweet_obj):\n",
    "    filtered_tweet_obj = {}\n",
    "    for field in tweet_obj:\n",
    "        field = field.decode(\"utf-8\")\n",
    "\n",
    "        if field == 'user':\n",
    "            user_obj = {}\n",
    "            for f in tweet_obj['user']:\n",
    "                if f in useful_user_fields:\n",
    "                    user_obj[f] = tweet_obj['user'][f]\n",
    "            filtered_tweet_obj['user']=user_obj\n",
    "        elif field in useful_tweet_fields:\n",
    "            filtered_tweet_obj[field]=tweet_obj[field]\n",
    "    \n",
    "    return filtered_tweet_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_id2tweet(threads, lvl_1_path, id2tweet):\n",
    "    for r_id in threads:\n",
    "        # for each rumour_thead, there is ONE source-tweet, and MULTIPLE reations\n",
    "\n",
    "        # {base}/{event}/{rumours or non-rumour)}/{thread-id}/source-tweets/{rumour-id}.json\n",
    "        with open('{}/{}/{}/{}/source-tweets/{}.json'.format(base_path, event, lvl_1_path, r_id, r_id)) as json_file:\n",
    "            tweet_json = json.load(json_file)\n",
    "            tweet_json = format_tweet(tweet_json)\n",
    "            id2tweet[tweet_json['id_str'].decode(\"utf-8\")] = tweet_json\n",
    "            \n",
    "        reaction_tweets = [d for d in os.listdir('{}/{}/{}/{}/reactions'.format(base_path, event, lvl_1_path, r_id))\n",
    "                          if 'json' in d]\n",
    "        for r_tweet_id in reaction_tweets:\n",
    "            with open('{}/{}/{}/{}/reactions/{}'.format(base_path, event, lvl_1_path, r_id, r_tweet_id)) as json_file:\n",
    "                tweet_json = json.load(json_file)\n",
    "                tweet_json = format_tweet(tweet_json)\n",
    "                id2tweet[tweet_json['id_str'].decode(\"utf-8\")] = tweet_json\n",
    "    return id2tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [d for d in os.listdir(base_path) if d!='.DS_Store']\n",
    "id2tweet = {}\n",
    "\n",
    "for event in tqdm(events):\n",
    "    threads = [d for d in os.listdir('{}/{}'.format(base_path, event))]\n",
    "    print(event)\n",
    "\n",
    "    rumour_threads = [d for d in os.listdir('{}/{}/rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "    non_rumour_threads = [d for d in os.listdir('{}/{}/non-rumours'.format(base_path, event)) if d != '.DS_Store']\n",
    "\n",
    "    id2tweet = process_id2tweet(rumour_threads, 'rumours', id2tweet)\n",
    "    id2tweet = process_id2tweet(non_rumour_threads, 'non-rumours', id2tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./id2tweet.pickle', 'wb') as handle:\n",
    "    pickle.dump(id2tweet, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
